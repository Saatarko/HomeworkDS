{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655ad485-1e0a-4839-8a27-b26c1a2c42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Отключает все логи TensorFlow (0 - все, 1 - предупреждения, 2 - ошибки, 3 - критичные ошибки)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faee1be-6764-4fc7-b6ab-628d3bedee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dill\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import time\n",
    "from gower import gower_matrix\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time  \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from dotenv import load_dotenv\n",
    "import neptune\n",
    "import optuna\n",
    "import time  \n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, mean_absolute_error, r2_score, mean_squared_error, make_scorer\n",
    "import shap\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sqlalchemy import Column, Integer, String, ForeignKey, create_engine, DateTime, func, Text, select\n",
    "from sqlalchemy import Float, Boolean\n",
    "from sqlalchemy.orm import declarative_base, Session, relationship, sessionmaker, configure_mappers\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, precision_score, recall_score, average_precision_score, f1_score, ndcg_score\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.utils import resample\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Embedding, Input, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, adjusted_rand_score\n",
    "from keras import backend as K  # Импортируем Keras backend\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938f50ad-820e-4246-962a-ad28d6c132a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'image', 'image_dataset_from_directory', 'sequence', 'text', 'text_dataset_from_directory', 'timeseries_dataset_from_array']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tf.keras.preprocessing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1b7d7-00de-4db6-bafd-6e750bddb00f",
   "metadata": {},
   "source": [
    "## План работы с рекомендательными системами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef77f2-c5f1-4e33-a730-78fb29265ba9",
   "metadata": {},
   "source": [
    "**Простые модели**\n",
    "- **Подготовка данных. Получение \"идельных рекомендаций\"**\n",
    "- **1 TF-IDF + overview Базовая модель**\n",
    "- **2 Сравнение метрик сходства Косинус, Jaccard, Евклид, Манхэттен**\n",
    "- **3 TF-IDF + дополнительные признаки (genres, tagline)**\n",
    "- **4 KNN Поиск ближайших соседей**\n",
    "- **5 ALS с добавлением взаимодействия (vote_count)**\n",
    "- **6 LDA (на overview) Тематическое моделирование**\n",
    "- **7 Гибридная модель: TF-IDF + популярность Учет vote_average, vote_count в финальном скоринге**\n",
    "  \n",
    "**Нейросети**\n",
    "- **8 Понижение размерности PCA/Autoencoder (на TF-IDF)  и обучение модели**\n",
    "- **9 Кластеризация (Agglomerative Clustering) Группировка фильмов**\n",
    "- **10 Финальный ансамбль Всё вместе: признаки + AE + кластеризация + скоринг**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9b8f1-5eda-4f67-ab40-c3c1072b5614",
   "metadata": {},
   "source": [
    "## Функция для получения \"идеальных рекомендаций\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccc2ac-a810-4942-b09c-254ec4fa2cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e2f7c7-30cb-4af9-b3ab-631e6c8a405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Загружен] Кэш на 1038 фильмов\n"
     ]
    }
   ],
   "source": [
    "# Путь к кэшу\n",
    "CACHE_PATH = 'content/recomend/imdb_recs_cache.pkl'\n",
    "\n",
    "# Проверка существования директории и её создание, если не существует\n",
    "cache_dir = os.path.dirname(CACHE_PATH)\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Загружаем кэш или создаем новый\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, 'rb') as f:\n",
    "        imdb_recs_cache = dill.load(f)\n",
    "    print(f\"[Загружен] Кэш на {len(imdb_recs_cache)} фильмов\")\n",
    "else:\n",
    "    imdb_recs_cache = {}\n",
    "    print(\"[Создан] Пустой кэш\")\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "def get_imdb_recommendations(imdb_id, delay=2, allow_scraping=True):\n",
    "    \"\"\"Парсит IMDb и возвращает список рекомендованных imdb_id (если разрешено)\"\"\"\n",
    "\n",
    "    # Уже есть в кэше — отдаем сразу\n",
    "    if imdb_id in imdb_recs_cache:\n",
    "        return imdb_recs_cache[imdb_id]\n",
    "\n",
    "    # Если нельзя парсить — возвращаем пусто\n",
    "    if not allow_scraping:\n",
    "        return []\n",
    "\n",
    "    url = f'https://www.imdb.com/title/{imdb_id}/'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"[Ошибка] Не удалось загрузить страницу для {imdb_id}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    recs = []\n",
    "\n",
    "    # Ищем все ссылки на похожие фильмы\n",
    "    rec_block = soup.find_all('a', href=True)\n",
    "    for a in rec_block:\n",
    "        href = a['href']\n",
    "        if href.startswith('/title/tt') and 'recommendations' not in href:\n",
    "            rec_id = href.split('/')[2]\n",
    "            if rec_id != imdb_id and rec_id.startswith('tt'):\n",
    "                recs.append(rec_id)\n",
    "\n",
    "    # Сохраняем только уникальные значения (до 10)\n",
    "    unique_recs = list(set(recs))[:10]\n",
    "    imdb_recs_cache[imdb_id] = unique_recs\n",
    "\n",
    "    # Сохраняем кэш каждые 5 фильмов\n",
    "    if len(imdb_recs_cache) % 5 == 0:\n",
    "        with open(CACHE_PATH, 'wb') as f:\n",
    "            dill.dump(imdb_recs_cache, f)\n",
    "        print(f\"[Сохранено] Кэш на {len(imdb_recs_cache)} фильмов\")\n",
    "\n",
    "    time.sleep(delay)\n",
    "    return unique_recs\n",
    "\n",
    "# Создаем ground_truth только для первых 100 фильмов\n",
    "def create_ground_truth(movies_data, limit=100):\n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Ограничиваем выборку до первых `limit` фильмов\n",
    "    movies_subset = movies_data.head(limit)\n",
    "    \n",
    "    # Для каждого фильма из subset получаем imdb_id\n",
    "    for imdb_id in movies_subset['imdb_id']:\n",
    "        # Получаем список рекомендаций для этого imdb_id\n",
    "        recs = get_imdb_recommendations(imdb_id)\n",
    "        if recs:\n",
    "            # Добавляем рекомендации в ground_truth\n",
    "            ground_truth[imdb_id] = recs\n",
    "    \n",
    "    return ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6e13b-e798-4af7-aaff-6fe99db12bea",
   "metadata": {},
   "source": [
    "## 1. TF-IDF + overview\tБазовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fb1229-d5b3-406d-a6e3-6d3350bfc375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "movies_metadata = pd.read_csv('content/movies_metadata.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffef1b90-bf11-4fd5-a405-8dfd36b29a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45466 entries, 0 to 45465\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   adult                  45466 non-null  object \n",
      " 1   belongs_to_collection  4494 non-null   object \n",
      " 2   budget                 45466 non-null  object \n",
      " 3   genres                 45466 non-null  object \n",
      " 4   homepage               7782 non-null   object \n",
      " 5   id                     45466 non-null  object \n",
      " 6   imdb_id                45449 non-null  object \n",
      " 7   original_language      45455 non-null  object \n",
      " 8   original_title         45466 non-null  object \n",
      " 9   overview               44512 non-null  object \n",
      " 10  popularity             45461 non-null  object \n",
      " 11  poster_path            45080 non-null  object \n",
      " 12  production_companies   45463 non-null  object \n",
      " 13  production_countries   45463 non-null  object \n",
      " 14  release_date           45379 non-null  object \n",
      " 15  revenue                45460 non-null  float64\n",
      " 16  runtime                45203 non-null  float64\n",
      " 17  spoken_languages       45460 non-null  object \n",
      " 18  status                 45379 non-null  object \n",
      " 19  tagline                20412 non-null  object \n",
      " 20  title                  45460 non-null  object \n",
      " 21  video                  45460 non-null  object \n",
      " 22  vote_average           45460 non-null  float64\n",
      " 23  vote_count             45460 non-null  float64\n",
      "dtypes: float64(4), object(20)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "movies_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "247ea54f-2b43-4350-af83-81eaf938df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_first = movies_metadata[['title', 'overview', 'imdb_id']].dropna(subset=['title', 'overview', 'imdb_id']).reset_index(drop=True)\n",
    "movies_first = movies_first[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a36174c0-7e85-4b4e-a540-471f10e14958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>imdb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>tt0114709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>tt0113497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>tt0113228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>tt0114885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>tt0113041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Él</td>\n",
       "      <td>Francisco is rich, rather strict on principles...</td>\n",
       "      <td>tt0045361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>La Chienne</td>\n",
       "      <td>Cashier Maurice Legrand is married to Adele, a...</td>\n",
       "      <td>tt0021739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Eréndira</td>\n",
       "      <td>While Erendira, a beautiful teenage girl, has ...</td>\n",
       "      <td>tt0085501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>The Private Lives of Elizabeth and Essex</td>\n",
       "      <td>This period drama frames the tumultuous affair...</td>\n",
       "      <td>tt0031826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>The Search</td>\n",
       "      <td>In post war Germany, a dislocated and orphaned...</td>\n",
       "      <td>tt0040765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0                                    Toy Story   \n",
       "1                                      Jumanji   \n",
       "2                             Grumpier Old Men   \n",
       "3                            Waiting to Exhale   \n",
       "4                  Father of the Bride Part II   \n",
       "...                                        ...   \n",
       "9995                                        Él   \n",
       "9996                                La Chienne   \n",
       "9997                                  Eréndira   \n",
       "9998  The Private Lives of Elizabeth and Essex   \n",
       "9999                                The Search   \n",
       "\n",
       "                                               overview    imdb_id  \n",
       "0     Led by Woody, Andy's toys live happily in his ...  tt0114709  \n",
       "1     When siblings Judy and Peter discover an encha...  tt0113497  \n",
       "2     A family wedding reignites the ancient feud be...  tt0113228  \n",
       "3     Cheated on, mistreated and stepped on, the wom...  tt0114885  \n",
       "4     Just when George Banks has recovered from his ...  tt0113041  \n",
       "...                                                 ...        ...  \n",
       "9995  Francisco is rich, rather strict on principles...  tt0045361  \n",
       "9996  Cashier Maurice Legrand is married to Adele, a...  tt0021739  \n",
       "9997  While Erendira, a beautiful teenage girl, has ...  tt0085501  \n",
       "9998  This period drama frames the tumultuous affair...  tt0031826  \n",
       "9999  In post war Germany, a dislocated and orphaned...  tt0040765  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccdccba-bd13-4286-b0f7-916d67602336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к файлу для сохранения ground_truth\n",
    "GROUND_TRUTH_PATH = 'content/recomend/ground_truth.pkl'\n",
    "IMDB_RECS_CACHE_PATH = 'content/recomend/imdb_recs_cache.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54103ed6-aa0b-4185-b061-98e62ba03421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка существования директории и её создание, если не существует\n",
    "cache_dir = os.path.dirname(GROUND_TRUTH_PATH)\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Загружаем или создаём ground_truth\n",
    "if os.path.exists(GROUND_TRUTH_PATH):\n",
    "    with open(GROUND_TRUTH_PATH, 'rb') as f:\n",
    "        ground_truth = dill.load(f)\n",
    "else:\n",
    "    ground_truth = create_ground_truth(movies_first, limit=100)\n",
    "    # Сохраняем ground_truth\n",
    "    with open(GROUND_TRUTH_PATH, 'wb') as f:\n",
    "        dill.dump(ground_truth, f)\n",
    "\n",
    "# Загружаем или создаём imdb_recs_cache\n",
    "if os.path.exists(IMDB_RECS_CACHE_PATH):\n",
    "    with open(IMDB_RECS_CACHE_PATH, 'rb') as f:\n",
    "        imdb_recs_cache = dill.load(f)\n",
    "else:\n",
    "    imdb_recs_cache = {}  # Создаём новый кэш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537c23b-418c-4883-98ba-c2df1a686f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ground_truth = create_ground_truth(movies_first, limit=100)\n",
    "# ground_truth_2 = create_ground_truth(movies_first, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bb74c-3be6-4ea1-be63-00a21ce3be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth = ground_truth_2\n",
    "# with open(GROUND_TRUTH_PATH, 'wb') as f:\n",
    "#     dill.dump(ground_truth, f)\n",
    "# with open(IMDB_RECS_CACHE_PATH, 'wb') as f:\n",
    "#     dill.dump(imdb_recs_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e725ed9-c879-4e5e-baaf-999b63ad59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация описаний\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_first['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f78df00-7997-449d-b191-3b4dbbf992c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет косинусного сходства\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbc2401-7c5a-41e3-bfd5-6c8256d0e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "# Путь к лог-файлу\n",
    "_PATH = 'content/recomend/log'\n",
    "os.makedirs(os.path.dirname(_PATH), exist_ok=True)\n",
    "\n",
    "def log_to_file(message):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(_PATH, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "\n",
    "\n",
    "# Функция для расчета регрессионных метрик\n",
    "def calculate_regression_metrics(predictions, ground_truth):\n",
    "    # Если в predictions или ground_truth есть строки, попробуем преобразовать их в числа\n",
    "    try:\n",
    "        predictions = np.array(predictions, dtype=float)\n",
    "        ground_truth = np.array(ground_truth, dtype=float)\n",
    "    except ValueError:\n",
    "        return float('nan'), float('nan')  # Если не удается преобразовать в числа, возвращаем NaN\n",
    "\n",
    "    if len(predictions) == 0 or len(ground_truth) == 0:\n",
    "        return float('nan'), float('nan')  # Возвращаем NaN, если нет данных\n",
    "\n",
    "    mse = mean_squared_error(ground_truth, predictions)\n",
    "    rmse = mean_squared_error(ground_truth, predictions, squared=False)  # sqrt(MSE)\n",
    "    return mse, rmse\n",
    "\n",
    "def get_recommendations(title, sim, movies, movies_metadata, imdb_recs_cache, top_k=10, allow_scraping=True):\n",
    "    try:\n",
    "        idx_list = movies.index[movies['title'] == title].tolist()\n",
    "        if not idx_list:\n",
    "            log_to_file(f\"⚠️ Фильм не найден в датасете: '{title}'\")\n",
    "            return []\n",
    "\n",
    "        idx = idx_list[0]\n",
    "\n",
    "        sim_scores = list(enumerate(sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:top_k+1]\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "        recommended_titles = movies['title'].iloc[movie_indices].tolist()\n",
    "\n",
    "        rec_imdb_ids = []\n",
    "        for rec_title in recommended_titles:\n",
    "            meta_row = movies_metadata[movies_metadata['title'] == rec_title]\n",
    "            if meta_row.empty or pd.isna(meta_row['imdb_id'].values[0]):\n",
    "                log_to_file(f\"ℹ️ Пропущено: нет imdb_id для '{rec_title}'\")\n",
    "                continue\n",
    "\n",
    "            imdb_id = meta_row['imdb_id'].values[0]\n",
    "            if imdb_id not in imdb_recs_cache:\n",
    "                if not allow_scraping:\n",
    "                    log_to_file(f\"[Пропущено] IMDb scraping выключен для '{rec_title}' (id: {imdb_id})\")\n",
    "                    continue\n",
    "                imdb_recs_cache[imdb_id] = get_imdb_recommendations(imdb_id, imdb_recs_cache, allow_scraping=True)\n",
    "\n",
    "            rec_imdb_ids.extend(imdb_recs_cache.get(imdb_id, []))\n",
    "\n",
    "        return rec_imdb_ids[:top_k]\n",
    "\n",
    "    except Exception as e:\n",
    "        log_to_file(f\"❌ Ошибка при обработке '{title}': {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "# Модель, возвращающая imdb_id по imdb_id\n",
    "def my_recommend_model(imdb_id, movies, movies_metadata, sim, imdb_recs_cache, top_k=10, allow_scraping=True):\n",
    "    title_row = movies[movies['imdb_id'] == imdb_id]\n",
    "    if title_row.empty:\n",
    "        return []\n",
    "    title = title_row['title'].values[0]\n",
    "    rec_imdb_ids = get_recommendations(title, sim, movies, movies_metadata, imdb_recs_cache, top_k, allow_scraping=allow_scraping)\n",
    "    return rec_imdb_ids\n",
    "\n",
    "\n",
    "# Метрики\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    pred_set = set(predictions)\n",
    "    true_set = set(ground_truth)\n",
    "    if not pred_set or not true_set:\n",
    "        return 0, 0, 0, 0, 0\n",
    "\n",
    "    y_true = [1 if item in true_set else 0 for item in predictions]\n",
    "    y_score = [1] * len(predictions)\n",
    "\n",
    "    precision = precision_score([1]*len(y_true), y_true, zero_division=0)\n",
    "    recall = recall_score([1]*len(y_true), y_true, zero_division=0)\n",
    "    f1 = f1_score([1]*len(y_true), y_true, zero_division=0)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "    # ✅ Проверка длины списка перед ndcg_score\n",
    "    if len(y_true) > 1:\n",
    "        ndcg = ndcg_score([y_true], [y_score])\n",
    "    else:\n",
    "        ndcg = 0.0\n",
    "        log_to_file(f\"NDCG пропущен из-за недостаточного количества рекомендаций: {len(y_true)}\")\n",
    "\n",
    "    return precision, recall, f1, ap, ndcg\n",
    "\n",
    "def evaluate_model(model_func, ground_truth, movies, movies_metadata, sim, imdb_recs_cache, top_k=10, is_regression=False, allow_scraping=False):\n",
    "    scores = {'precision': [], 'recall': [], 'f1': [], 'ap': [], 'ndcg': []}\n",
    "\n",
    "    if is_regression:\n",
    "        scores['mse'] = []\n",
    "        scores['rmse'] = []\n",
    "\n",
    "    for imdb_id, true_ids in tqdm(ground_truth.items(), desc=\"Оценка модели\"):\n",
    "        # Только если этот фильм есть в нашей базе\n",
    "        if imdb_id not in movies['imdb_id'].values:\n",
    "            continue\n",
    "\n",
    "        # Получаем рекомендации по модели\n",
    "        preds = model_func(imdb_id, movies, movies_metadata, sim, imdb_recs_cache, top_k=top_k, allow_scraping=allow_scraping)\n",
    "\n",
    "        if is_regression:\n",
    "            # Если рекомендации пусты, пропускаем этот случай\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "            # Если это регрессия, то вычисляем MSE и RMSE\n",
    "            mse, rmse = calculate_regression_metrics(preds, true_ids)\n",
    "            scores['mse'].append(mse)\n",
    "            scores['rmse'].append(rmse)\n",
    "        else:\n",
    "            # Для классификации считаем precision, recall и другие метрики\n",
    "            precision, recall, f1, ap, ndcg = calculate_metrics(preds, true_ids)\n",
    "            scores['precision'].append(precision)\n",
    "            scores['recall'].append(recall)\n",
    "            scores['f1'].append(f1)\n",
    "            scores['ap'].append(ap)\n",
    "            scores['ndcg'].append(ndcg)\n",
    "\n",
    "    # Возвращаем средние значения метрик\n",
    "    result = {k: round(sum(v) / len(v), 4) if v else 0.0 for k, v in scores.items()}\n",
    "    return result\n",
    "\n",
    "def evaluate_model_als(model_func, ground_truth, movies, movies_metadata, sim, imdb_recs_cache, top_k=10, is_regression=False, allow_scraping=False):\n",
    "    scores = {'precision': [], 'recall': [], 'f1': [], 'ap': [], 'ndcg': []}\n",
    "\n",
    "    if is_regression:\n",
    "        scores['mse'] = []\n",
    "        scores['rmse'] = []\n",
    "\n",
    "    for imdb_id, true_ids in tqdm(ground_truth.items(), desc=\"Оценка модели\"):\n",
    "        # Только если этот фильм есть в нашей базе\n",
    "        if imdb_id not in movies['imdb_id'].values:\n",
    "            continue\n",
    "\n",
    "        # Получаем рекомендации по модели\n",
    "        preds = model_func(imdb_id, movies, movies_metadata, sim, imdb_recs_cache, allow_scraping=allow_scraping)\n",
    "\n",
    "        if is_regression:\n",
    "            # Если рекомендации пусты, пропускаем этот случай\n",
    "            if len(preds) == 0:\n",
    "                continue\n",
    "            # Если это регрессия, то вычисляем MSE и RMSE\n",
    "            mse, rmse = calculate_regression_metrics(preds, true_ids)\n",
    "            scores['mse'].append(mse)\n",
    "            scores['rmse'].append(rmse)\n",
    "        else:\n",
    "            # Для классификации считаем precision, recall и другие метрики\n",
    "            precision, recall, f1, ap, ndcg = calculate_metrics(preds, true_ids)\n",
    "            scores['precision'].append(precision)\n",
    "            scores['recall'].append(recall)\n",
    "            scores['f1'].append(f1)\n",
    "            scores['ap'].append(ap)\n",
    "            scores['ndcg'].append(ndcg)\n",
    "\n",
    "    # Возвращаем средние значения метрик\n",
    "    result = {k: round(sum(v) / len(v), 4) if v else 0.0 for k, v in scores.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6e63e0e-453a-4fdc-b5a2-e2cb82b23151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:29<00:00, 33.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.0576, 'recall': 0.0319, 'f1': 0.036, 'ap': 0.0319, 'ndcg': 0.0431}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка ground_truth\n",
    "with open('content/recomend/ground_truth.pkl', 'rb') as f:\n",
    "    ground_truth = dill.load(f)\n",
    "\n",
    "# Оценка модели\n",
    "baseline_metrics = evaluate_model(my_recommend_model, ground_truth, movies_first, movies_metadata, cosine_sim, imdb_recs_cache, top_k=10)\n",
    "\n",
    "# Вывод метрик\n",
    "print(baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018e1d1-dafc-4acb-b0d5-d22242b19f07",
   "metadata": {},
   "source": [
    "### Метрики совсем плохии. Чтомы и видели собсвтенными глазами. Так что не удивительно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f896b9-8096-48e3-a9b9-0463530c7dd2",
   "metadata": {},
   "source": [
    "## 2. Сравнение метрик сходства\tКосинус, Jaccard, Евклид, Манхэттен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9a50d-ecdc-432b-a945-bab764a5c5c7",
   "metadata": {},
   "source": [
    "1. Косинусное сходство (Cosine Similarity)\n",
    "Определение: Косинусное сходство измеряет угол между двумя векторами в многомерном пространстве. Оно не учитывает длину векторов, а только их направление. Это сходство особенно полезно при работе с текстовыми данными (например, с TF-IDF векториализацией), где важен только общий паттерн слов, а не их абсолютные частоты.\n",
    "\n",
    "Формула:\n",
    "Cosine Similarity(A, B) = (A · B) / (||A|| ||B||)\n",
    "где:\n",
    "A · B — скалярное произведение векторов A и B,\n",
    "||A|| и ||B|| — нормы (длины) векторов A и B.\n",
    "Диапазон значений: от -1 до 1\n",
    "1 означает, что вектора одинаковы по направлению,\n",
    "0 — что вектора ортогональны (не имеют общего направления),\n",
    "-1 — что вектора противоположны.\n",
    "\n",
    "Применение:\n",
    "В основном используется для текстовых данных (например, в поисковых системах, рекомендательных системах и анализе текстов).\n",
    "Хорошо работает, когда важно сравнивать схожесть вектора признаков (например, слова или документы).\n",
    "\n",
    "2. Сходство Жаккара (Jaccard Similarity)\n",
    "Определение: Сходство Жаккара измеряет схожесть двух наборов, как отношение количества общих элементов к количеству всех элементов, которые встречаются хотя бы в одном из наборов. Это мера, используемая для сравнения наборов (множеств) объектов.\n",
    "\n",
    "Формула:\n",
    "Jaccard Similarity(A, B) = |A ∩ B| / |A ∪ B|\n",
    " \n",
    "где:\n",
    "|A ∩ B| — количество общих элементов в множествах A и B,\n",
    "|A ∪ B| — количество всех уникальных элементов в объединении множеств A и B.\n",
    "\n",
    "Диапазон значений: от 0 до 1\n",
    "1 означает, что наборы идентичны,\n",
    "0 означает, что наборы не пересекаются.\n",
    "\n",
    "Применение:\n",
    "Применяется для бинарных данных, например, для сравнения наличия/отсутствия признаков (например, в рекомендательных системах для сравнения пользовательских предпочтений или товаров).\n",
    "Хорошо работает для категориальных данных, где важен только факт наличия элементов, а не их количество.\n",
    "\n",
    "3. Евклидово расстояние (Euclidean Distance)\n",
    "Определение: Евклидово расстояние — это стандартная метрика для измерения прямого расстояния между двумя точками в многомерном пространстве. В отличие от косинусного сходства, оно чувствительно к разнице в величинах признаков.\n",
    "\n",
    "Формула:\n",
    "\n",
    "Euclidean Distance(A, B) = sqrt( Σ (A_i - B_i)^2 )\n",
    "где:\n",
    "A_i и B_i — элементы векторов A и B,\n",
    "Σ — суммирование по всем признакам.\n",
    "Диапазон значений: от 0 до ∞\n",
    "0 означает, что точки совпадают,\n",
    "большие значения означают большую разницу между точками.\n",
    "\n",
    "Применение:\n",
    "Используется в задачах кластеризации, например, в методах, таких как K-Means или K-Nearest Neighbors (KNN).\n",
    "Хорошо работает для числовых данных, где значения признаков важны и влияют на сходство.\n",
    "\n",
    "4. Манхэттенское расстояние (Manhattan Distance)\n",
    "Определение: Манхэттенское расстояние (или таксистское расстояние) измеряет расстояние между двумя точками, двигаясь по осям (т.е. только горизонтально и вертикально, как если бы вы передвигались по прямым улицам города).\n",
    "\n",
    "Формула:\n",
    "\n",
    "Manhattan Distance(A, B) = Σ |A_i - B_i|\n",
    "где:\n",
    "A_i и B_i — элементы векторов A и B,\n",
    "Σ — суммирование по всем признакам.\n",
    "\n",
    "Диапазон значений: от 0 до ∞\n",
    "0 означает, что точки совпадают,\n",
    "большие значения означают большую разницу между точками.\n",
    "\n",
    "Применение:\n",
    "Используется в задачах, где важно учитывать только перемещения по осям, например, в некоторых видах кластеризации и моделях классификации.\n",
    "Подходит для случаев, когда изменение в одном признаке не имеет такого же эффекта, как изменение в другом (в отличие от евклидового расстояния).\n",
    "\n",
    "Когда использовать каждую метрику:\n",
    "Косинусное сходство полезно, когда важен угол между объектами, например, в текстовых данных или когда важно понять сходство между объектами, независимо от их масштабов.\n",
    "Сходство Жаккара отлично подходит для бинарных данных, когда важно учитывать только факт наличия или отсутствия признаков.\n",
    "Евклидово расстояние лучше подходит для числовых данных, когда важно учитывать прямое расстояние между объектами.\n",
    "Манхэттенское расстояние удобно, когда вы работаете с многими осьми, и вам важно учитывать движение вдоль отдельных осей, а не на прямую линию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc5807b1-bd59-4193-a252-a2b9264d07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем список метрик сходства\n",
    "similarity_metrics = {\n",
    "    'cosine': cosine_similarity(tfidf_matrix),\n",
    "    'euclidean': euclidean_distances(tfidf_matrix),\n",
    "    'manhattan': manhattan_distances(tfidf_matrix)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1a7fc89-e8c4-4113-bfcc-259c25db970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели для метрики: cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:29<00:00, 33.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики cosine:\n",
      "{'precision': 0.0576, 'recall': 0.0319, 'f1': 0.036, 'ap': 0.0319, 'ndcg': 0.0431}\n",
      "--------------------------------------------------\n",
      "Оценка модели для метрики: euclidean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:25<00:00, 38.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики euclidean:\n",
      "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'ap': 0.0, 'ndcg': 0.0, 'mse': nan, 'rmse': nan}\n",
      "--------------------------------------------------\n",
      "Оценка модели для метрики: manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:27<00:00, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики manhattan:\n",
      "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'ap': 0.0, 'ndcg': 0.0, 'mse': 0.0, 'rmse': 0.0}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели с различными метриками\n",
    "for metric_name, sim_matrix in similarity_metrics.items():\n",
    "    print(f\"Оценка модели для метрики: {metric_name}\")\n",
    "    \n",
    "    # Определяем, нужно ли использовать регрессионные метрики\n",
    "    is_regression = metric_name in ['euclidean', 'manhattan']  # Для евклидова и манхэттенского расстояний используем регрессионные метрики\n",
    "\n",
    "    # Оценка модели\n",
    "    metrics_result = evaluate_model(\n",
    "        my_recommend_model, \n",
    "        ground_truth, \n",
    "        movies_first, \n",
    "        movies_metadata, \n",
    "        sim_matrix, \n",
    "        imdb_recs_cache, \n",
    "        top_k=10, \n",
    "        is_regression=is_regression\n",
    "    )\n",
    "    \n",
    "    # Вывод метрик\n",
    "    print(f\"Результаты для метрики {metric_name}:\")\n",
    "    print(metrics_result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903aae0-bf8d-4280-8092-e981d785cd84",
   "metadata": {},
   "source": [
    "### Все варианты расчетов показали себя хуже косинусов. Что тоже логично т.к косинусы больше всех подхзодят к рекомендациям по текстам. Поэтому пока их и оставим"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb21ac6-e409-4b96-9ec0-8b1501df2706",
   "metadata": {},
   "source": [
    "## 3 TF-IDF + дополнительные признаки (genres, tagline) \"Мешок признаков\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92053e1c-6873-4d7b-a717-ba73c9f5e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_second = movies_metadata[['title', 'overview', 'imdb_id', 'tagline' , 'genres' ]].reset_index(drop=True)\n",
    "movies_second = movies_second[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e9692f0-0cf6-43d0-bce0-0b1bc1e687f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>tagline</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview    imdb_id  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  tt0114709   \n",
       "1  When siblings Judy and Peter discover an encha...  tt0113497   \n",
       "2  A family wedding reignites the ancient feud be...  tt0113228   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  tt0114885   \n",
       "4  Just when George Banks has recovered from his ...  tt0113041   \n",
       "\n",
       "                                             tagline  \\\n",
       "0                                                NaN   \n",
       "1          Roll the dice and unleash the excitement!   \n",
       "2  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Friends are the people who let you be yourself...   \n",
       "4  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                                              genres  \n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...  \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...  \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...  \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...  \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_second.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a15f395-e3c8-4a6e-a94b-5ca735f7d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Объединяем все признаки в одну строку\n",
    "movies_second['combined'] = movies_second['overview'] + \" \" + movies_second['tagline'] + \" \" + movies_second['genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 2. Заполняем пропущенные значения в 'combined' пустой строкой\n",
    "movies_second['combined'] = movies_second['combined'].fillna('')\n",
    "\n",
    "# 3. Векторизация всех признаков с помощью TfidfVectorizer\n",
    "tfidf_vectorizer_second = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix_second = tfidf_vectorizer_second.fit_transform(movies_second['combined'])\n",
    "\n",
    "# 3. Вычисляем сходства для различных метрик\n",
    "similarity_metrics_second = {\n",
    "    'cosine': cosine_similarity(tfidf_matrix_second),\n",
    "    'euclidean': euclidean_distances(tfidf_matrix_second),\n",
    "    'manhattan': manhattan_distances(tfidf_matrix_second)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6398bae9-3e87-4a02-adae-b54dcff6298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка модели для метрики: cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:29<00:00, 33.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики cosine:\n",
      "{'precision': 0.0071, 'recall': 0.0012, 'f1': 0.002, 'ap': 0.0012, 'ndcg': 0.0037}\n",
      "--------------------------------------------------\n",
      "Оценка модели для метрики: euclidean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:25<00:00, 38.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики euclidean:\n",
      "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'ap': 0.0, 'ndcg': 0.0, 'mse': nan, 'rmse': nan}\n",
      "--------------------------------------------------\n",
      "Оценка модели для метрики: manhattan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:27<00:00, 36.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты для метрики manhattan:\n",
      "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'ap': 0.0, 'ndcg': 0.0, 'mse': nan, 'rmse': nan}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели с различными метриками\n",
    "for metric_name, sim_matrix in similarity_metrics_second.items():\n",
    "    print(f\"Оценка модели для метрики: {metric_name}\")\n",
    "    \n",
    "    # Определяем, нужно ли использовать регрессионные метрики\n",
    "    is_regression = metric_name in ['euclidean', 'manhattan']  # Для евклидова и манхэттенского расстояний используем регрессионные метрики\n",
    "\n",
    "    # Оценка модели\n",
    "    metrics_result = evaluate_model(\n",
    "        my_recommend_model, \n",
    "        ground_truth, \n",
    "        movies_first, \n",
    "        movies_metadata, \n",
    "        sim_matrix, \n",
    "        imdb_recs_cache, \n",
    "        top_k=10, \n",
    "        is_regression=is_regression\n",
    "    )\n",
    "    \n",
    "    # Вывод метрик\n",
    "    print(f\"Результаты для метрики {metric_name}:\")\n",
    "    print(metrics_result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531417c-19b7-40d7-ab98-020b635adb24",
   "metadata": {},
   "source": [
    "### Пока что наша модель проста, и, возможно, для улучшения метрик потребуется переход на более сложные модели и методы, такие как использование дополнительных фичей, более мощных алгоритмов (например, нейронных сетей), а также улучшение качества данных (например, очистка данных, улучшение предобработки текста. Т.к сейчас дополнителиные фичи только ухудшили результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db25088-c078-4186-adfe-c64be36b2972",
   "metadata": {},
   "source": [
    "## 4. KNN Поиск ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787ecbf-60be-48ce-96e1-8632712adbd0",
   "metadata": {},
   "source": [
    "KNN (K-Nearest Neighbors)\n",
    "- Принцип работы:\n",
    "KNN — это метод, основанный на идее поиска ближайших соседей. Для рекомендации товаров или фильмов он ищет пользователей или элементы, которые наиболее похожи на заданного пользователя или фильм. Основные шаги:\n",
    "- Преобразование данных:\n",
    "Мы представляем каждый фильм или пользователя в виде вектора признаков (например, с использованием TF-IDF, меток жанра или других характеристик).\n",
    "Расстояние между объектами:\n",
    "Чтобы найти «похожие» объекты, мы измеряем расстояние (или схожесть) между векторами с использованием таких метрик, как евклидово расстояние, косинусная схожесть и другие.\n",
    "- Поиск ближайших соседей:\n",
    "Для конкретного пользователя или фильма KNN находит ближайших соседей (фильмов или пользователей), основываясь на выбранной метрике сходства.\n",
    "- Рекомендация:\n",
    "Для фильма или пользователя на основе схожести с другими объектами рекомендатор предсказывает, какие элементы он мог бы \"нравиться\" или какие фильмы могут ему подойти.\n",
    "Физический смысл:\n",
    "Модель ищет ближайших соседей на основе сходства признаков. Например, если фильм похож на другие по жанрам, актёрам, рейтингу, то пользователю могут порекомендовать эти фильмы. KNN можно представить как способ поиск похожих объектов в большом множестве данных.\n",
    "- Преимущества:\n",
    "Простота и интерпретируемость.\n",
    "Модель не требует обучения, потому что она просто использует данные для поиска ближайших соседей.\n",
    "- Недостатки:\n",
    "Высокая вычислительная нагрузка на больших данных, особенно при поиске по всей базе.\n",
    "Не всегда подходит для более сложных или скрытых зависимостей между объектами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbc4337e-18c7-4086-8899-1869eca625da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;NearestNeighbors<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.NearestNeighbors.html\">?<span>Documentation for NearestNeighbors</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=10)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10  # Количество ближайших соседей\n",
    "\n",
    "# Инициализируем модель KNN с косинусным сходством\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "\n",
    "# Обучаем модель на tfidf_matrix\n",
    "knn.fit(tfidf_matrix_second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ef15d8d-c263-4da7-aa73-e6ec2b782322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновляем функцию для получения рекомендаций через KNN\n",
    "def get_knn_recommendations(imdb_id, movies, knn_model, movies_metadata, imdb_recs_cache, top_k=10, allow_scraping=True):\n",
    "    title_row = movies[movies['imdb_id'] == imdb_id]\n",
    "    if title_row.empty:\n",
    "        return []\n",
    "\n",
    "    title = title_row['title'].values[0]\n",
    "    idx_list = movies.index[movies['title'] == title].tolist()\n",
    "    if not idx_list:\n",
    "        return []\n",
    "    \n",
    "    idx = idx_list[0]\n",
    "    distances, indices = knn_model.kneighbors(tfidf_matrix_second[idx], n_neighbors=top_k+1)\n",
    "\n",
    "    recommended_titles = movies['title'].iloc[indices[0][1:]].tolist()  # Пропускаем первый (сам себя)\n",
    "    rec_imdb_ids = []\n",
    "\n",
    "    for rec_title in recommended_titles:\n",
    "        meta_row = movies_metadata[movies_metadata['title'].str.strip().str.lower() == rec_title.strip().lower()]\n",
    "        if not meta_row.empty:\n",
    "            imdb_id_value = meta_row['imdb_id'].values[0]\n",
    "            rec_imdb_ids.append(imdb_id_value)\n",
    "        else:\n",
    "            log_to_file(f\"ℹ️ Пропущено: нет imdb_id для '{rec_title}'\")\n",
    "\n",
    "    return rec_imdb_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "742dd0a9-50dc-4c3c-a5f8-a5dafaf70b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [02:06<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты оценки модели KNN:\n",
      "{'precision': 0.0899, 'recall': 0.0117, 'f1': 0.0201, 'ap': 0.0117, 'ndcg': 0.0434}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Оценка модели с помощью KNN (без allow_scraping)\n",
    "results_knn = evaluate_model(\n",
    "    get_knn_recommendations,\n",
    "    ground_truth,\n",
    "    movies_second,\n",
    "    knn,                # <--- Сначала knn модель\n",
    "    movies_metadata,    # <--- Потом метаданные\n",
    "    imdb_recs_cache,\n",
    "    top_k=10,\n",
    "    is_regression=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Результаты оценки модели KNN:\")\n",
    "print(results_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6c614-e67b-4d64-90d2-4f562f457d64",
   "metadata": {},
   "source": [
    "### Результат все еще очень низкий но лучше даже чем на чистом описании. Т.е усложняя можель добавление новых признаков- всеже улучшает резульата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb364f4-043b-478b-972a-9ae986f02ba8",
   "metadata": {},
   "source": [
    "## 5 ALS с добавлением взаимодействия (vote_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc325c-6cf1-4e7c-af38-90810eb8d51d",
   "metadata": {},
   "source": [
    "ALS (Alternating Least Squares)\n",
    "- Принцип работы:\n",
    "ALS — это метод матричной факторизации, который активно используется для рекомендательных систем, особенно в тех случаях, когда у нас есть большая матрица взаимодействий между пользователями и объектами (например, фильмы и оценки).\n",
    "- Матрица взаимодействий:\n",
    "Есть большая матрица, где строки — это пользователи, а столбцы — это объекты (например, фильмы). Каждое значение в ячейке — это взаимодействие пользователя с объектом (например, оценка фильма).\n",
    "- Матричная факторизация:\n",
    "ALS пытается разложить эту матрицу на две меньшие матрицы: одну для пользователей и одну для объектов. Эти матрицы называются факторами.\n",
    "Идея заключается в том, чтобы найти скрытые представления (факторы), которые могут объяснить взаимодействия между пользователями и объектами. Модель ищет структуру в данных и пытается понять, какие скрытые факторы (например, предпочтения пользователя, жанры фильмов) влияют на взаимодействия.\n",
    "- Чередующиеся минимизации (Alternating):\n",
    "ALS поочередно фиксирует одну из матриц (например, матрицу факторов пользователей) и минимизирует ошибку по другой (например, матрице факторов объектов), и наоборот. Это называется чередующейся минимизацией (alternating minimization).\n",
    "Алгоритм выполняет итерации, обновляя матрицы, пока ошибка (разница между предсказанными и реальными оценками) не станет минимальной.\n",
    "- Физический смысл:\n",
    "Это метод выделения скрытых факторов, которые объясняют поведение пользователей. В контексте фильмов это может быть жанровые предпочтения, актерский состав, тематика или другие факторы, которые влияют на предпочтения.\n",
    "Таким образом, ALS пытается понять, что пользователи «ценят» в фильмах, и на основе этого делать предсказания.\n",
    "- Преимущества:\n",
    "Эффективен для работы с большими наборами данных, где взаимодействий (оценок) немного.\n",
    "Подходит для задач, где важно выделить скрытые зависимости между пользователями и объектами.\n",
    "- Недостатки:\n",
    "Требует времени на обучение и вычисления, особенно для больших данных.\n",
    "Может давать плохие результаты, если данные плохо распределены или если данные разрежены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba21435f-2123-4933-a67c-4ef88b58a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_3 = movies_metadata[['title', 'overview', 'imdb_id', 'tagline' , 'genres', 'vote_average' , 'vote_count' ]].reset_index(drop=True)\n",
    "movies_3 = movies_3[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5330979-03d4-4eb9-ba62-5c06638f5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Создаем фиктивных пользователей\n",
    "movies_3 = movies_3.reset_index(drop=True)\n",
    "movies_3['user_id'] = movies_3.index\n",
    "movies_3['item_id'] = movies_3.index  # item_id тоже как индекс\n",
    "\n",
    "# 2. Взаимодействия на основе vote_count\n",
    "interactions_df = movies_3[['user_id', 'item_id', 'vote_count']].copy()\n",
    "\n",
    "# 3. Построим sparse матрицу (user-item matrix)\n",
    "user_item_matrix = coo_matrix(\n",
    "    (interactions_df['vote_count'].astype(float),\n",
    "     (interactions_df['user_id'], interactions_df['item_id']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2eb25c54-6f41-469a-9623-30a135d6522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saatarko/.conda/envs/HomeworkDS/lib/python3.12/site-packages/implicit/cpu/als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225608e9c40a49549a869c6c19ffaa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Шаг 1: Импорт и обучение ALS\n",
    "\n",
    "# Увеличиваем vote_count логарифмически, чтобы не было перекоса\n",
    "interactions_df['weight'] = np.log1p(interactions_df['vote_count'])\n",
    "\n",
    "# Пересоздаем sparse матрицу с весами\n",
    "user_item_matrix_weighted = coo_matrix(\n",
    "    (interactions_df['weight'],\n",
    "     (interactions_df['user_id'], interactions_df['item_id']))\n",
    ")\n",
    "\n",
    "# Преобразуем в формат CSR для обучения\n",
    "user_item_csr = user_item_matrix_weighted.tocsr()\n",
    "\n",
    "# Инициализируем и обучаем ALS\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=50,\n",
    "    regularization=0.1,\n",
    "    iterations=20,\n",
    "    use_gpu=False  # True если CUDA доступен\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "als_model.fit(user_item_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35fb45e9-5e61-4269-84c3-c0396636eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 рекомендаций для user_0: (array([8180, 1508, 1258, 6562, 1640, 4112, 5553, 6740, 3183,  582],\n",
      "      dtype=int32), array([0.14401034, 0.14005122, 0.13738999, 0.13507517, 0.13423133,\n",
      "       0.12423028, 0.1165394 , 0.1157335 , 0.11063281, 0.10995509],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Шаг 2: Получение рекомендаций для одного юзера\n",
    "user_id = 0\n",
    "recommended = als_model.recommend(user_id, user_item_csr[user_id], N=10)\n",
    "print(\"Top-10 рекомендаций для user_0:\", recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "48092d74-5ec7-4544-8e9c-ab3bcecc882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем в csr_matrix\n",
    "user_item_csr = user_item_matrix.tocsr()\n",
    "\n",
    "def get_als_recommendations(imdb_id, movies, als_model, user_item_csr, movies_metadata, imdb_recs_cache=None, allow_scraping=False):\n",
    "    # Преобразуем imdb_id в строку для безопасной работы с кэшем\n",
    "    imdb_id = str(imdb_id)\n",
    "    \n",
    "    # Убедимся, что imdb_recs_cache имеет корректные ключи\n",
    "    if not isinstance(imdb_recs_cache, dict):\n",
    "        imdb_recs_cache = {}\n",
    "\n",
    "    # Проверяем, есть ли уже рекомендации для данного imdb_id в кэше\n",
    "    if imdb_id not in imdb_recs_cache:\n",
    "        imdb_recs_cache[imdb_id] = []\n",
    "    \n",
    "    # Находим индекс фильма в DataFrame\n",
    "    movie_index = movies[movies['imdb_id'] == imdb_id].index[0]\n",
    "    \n",
    "    # Используем индекс фильма как идентификатор пользователя (синтетический)\n",
    "    user_id = movie_index  \n",
    "    \n",
    "    # Получаем recommendations, используя top_k из функции evaluate_model\n",
    "    recommended = als_model.recommend(user_id, user_item_csr[user_id], N=11)  # 10 + 1, чтобы исключить сам фильм\n",
    "    \n",
    "    # recommended состоит из двух массивов: item_ids и scores\n",
    "    item_ids, scores = recommended  # Это два отдельных массива\n",
    "    \n",
    "    # Исключаем сам фильм, если он в списке рекомендаций\n",
    "    rec_ids = [item_id for item_id in item_ids if item_id != user_id]\n",
    "    \n",
    "    # Получаем imdb_id для рекомендуемых фильмов\n",
    "    rec_imdb_ids = movies['imdb_id'].iloc[rec_ids].tolist()\n",
    "    \n",
    "    return rec_imdb_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d4bee90c-e8be-4006-b056-830d4bdefac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:05<00:00, 171.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Результаты оценки ALS:\n",
      "{'precision': 0.0273, 'recall': 0.0025, 'f1': 0.0045, 'ap': 0.0025, 'ndcg': 0.012}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Переход к оценке модели\n",
    "results_als = evaluate_model_als(\n",
    "    get_als_recommendations,\n",
    "    ground_truth,\n",
    "    movies_3,\n",
    "    als_model,\n",
    "    user_item_csr,\n",
    "    movies_metadata,\n",
    "    imdb_recs_cache,\n",
    "    is_regression=False,\n",
    "    allow_scraping=False\n",
    ")\n",
    "\n",
    "print(\"🎯 Результаты оценки ALS:\")\n",
    "print(results_als)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55844ba-c2b6-469e-9385-257f40b7aa08",
   "metadata": {},
   "source": [
    "### Полученные метрики хуже чем у KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7b930-dbed-40cb-aa90-2855384c3cd5",
   "metadata": {},
   "source": [
    "## 6. LDA (на overview) Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be222c4-70e6-42e0-b876-d5b6fafa90d3",
   "metadata": {},
   "source": [
    "LDA (Latent Dirichlet Allocation) — это алгоритм для тематического моделирования, который используется для выделения скрытых тем в текстовых данных. Он помогает понять, какие темы присутствуют в коллекции документов, и какие слова наиболее характерны для этих тем. LDA является вероятностной моделью, которая основывается на статистическом подходе.\n",
    "\n",
    "- Принцип работы LDA:\n",
    "LDA — это генеративная модель, которая предполагает следующее:\n",
    "Каждый документ (например, обзор фильма) является смесью нескольких тем.\n",
    "Каждая тема является распределением по словам (например, слова, связанные с любовью, драмой или фантастикой для фильмов).\n",
    "Задача LDA заключается в том, чтобы выявить эти скрытые темы, которые, вероятно, объясняют содержание документа.\n",
    "Алгоритм состоит из следующих этапов:\n",
    "\n",
    "- Предположения:\n",
    "Каждый документ представляет собой смесь нескольких скрытых тем.\n",
    "Каждая тема представляется распределением по словам.\n",
    "Процесс генерации документа (генеративная модель):\n",
    "Выбирается тема (из заранее заданного количества тем K).\n",
    "Для каждой темы выбирается слово из темы в соответствии с вероятностным распределением для этой темы.\n",
    "Этот процесс повторяется для всех слов в документе.\n",
    "- Обучение модели:\n",
    "Алгоритм пытается найти наиболее вероятные темы, которые могли бы привести к набору слов в документах. Это происходит через итерации, где на каждом шаге алгоритм обновляет вероятности того, какая тема порождает какое слово в документе, и каким образом темы распределяются по документам.\n",
    "Распределение по темам: После обучения модели, для каждого документа мы получаем распределение тем — то есть, сколько процентов каждого из K возможных тем есть в этом документе. Это позволяет выявить ключевые темы каждого документа (например, фильмы о любви, научной фантастике, комедии).\n",
    "- Ключевые элементы в LDA:\n",
    "Темы (Topics) — наборы слов, которые часто встречаются вместе в текстах. Каждая тема характеризуется распределением вероятности по словам (например, для темы \"спорт\" это будут слова: \"игра\", \"команда\", \"футбол\" и т.д.).\n",
    "Документы (Documents) — наборы слов. Документ может содержать несколько тем с разными вероятностями (например, обзор фильма может быть смесью тем \"драма\" и \"приключения\").\n",
    "Алгоритм — LDA использует методы скрытого распределения и Байесовскую индукцию для нахождения скрытых тем в тексте.\n",
    "- Зачем использовать LDA:\n",
    "Поиск скрытых тем в текстах: LDA помогает анализировать текстовые данные, извлекая темы, которые могут быть важными для понимания содержимого документа или коллекции документов. Это полезно, например, для классификации текстов по темам или улучшения поисковых систем.\n",
    "Снижение размерности: Множество текстов, например, 5000 слов в обзоре фильма, может быть сведено к нескольким скрытым темам, что позволяет уменьшить количество признаков и улучшить обработку данных для дальнейших задач, таких как кластеризация или рекомендации.\n",
    "Выявление паттернов: Это помогает находить связи и паттерны в данных, которые не очевидны на первый взгляд.\n",
    "- Пример:\n",
    "Предположим, у нас есть коллекция фильмов с их обзорами. С помощью LDA мы можем выделить несколько тем, например:\n",
    "Тема 1: Любовь, романтика, отношения (слова: любовь, роман, отношения, сердце)\n",
    "Тема 2: Приключения, экшн (слова: приключения, опасность, борьба, спасение)\n",
    "Тема 3: Научная фантастика, будущее (слова: космос, технологии, будущее, исследование)\n",
    "Каждый фильм будет иметь распределение этих тем, например:\n",
    "Фильм 1: 70% \"Любовь\", 20% \"Приключения\", 10% \"Научная фантастика\"\n",
    "Фильм 2: 10% \"Любовь\", 60% \"Приключения\", 30% \"Научная фантастика\"\n",
    "Так, LDA помогает нам понять, что скрыто в текстах, и создать более осмысленные рекомендации или аналитику.\n",
    "- Важные параметры в LDA:\n",
    "n_components — количество тем, которое вы хотите получить. Это параметр, который вы выбираете до начала обучения модели.\n",
    "max_iter — количество итераций для обучения модели.\n",
    "learning_decay и learning_offset — параметры, контролирующие скорость обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0451f989-b19c-4671-b9f8-e95c3913980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема #1:\n",
      "war film young life world story man love french new\n",
      "\n",
      "Тема #2:\n",
      "new detective agent police killer young murder cop york secret\n",
      "\n",
      "Тема #3:\n",
      "king arthur story love young based prince throne novel clerk\n",
      "\n",
      "Тема #4:\n",
      "love young man story family stolen new life film american\n",
      "\n",
      "Тема #5:\n",
      "documentary film life godzilla music comedy town story world young\n",
      "\n",
      "Тема #6:\n",
      "war world film man young group evil new life story\n",
      "\n",
      "Тема #7:\n",
      "life love young man family new woman father wife mother\n",
      "\n",
      "Тема #8:\n",
      "world war crew mysterious planet british film ship ii american\n",
      "\n",
      "Тема #9:\n",
      "life new school young man high old drug town love\n",
      "\n",
      "Тема #10:\n",
      "germany nazi justice communist hitler killer chip eugene dante king\n",
      "\n",
      "                         title  \\\n",
      "0                    Toy Story   \n",
      "1                      Jumanji   \n",
      "2             Grumpier Old Men   \n",
      "3            Waiting to Exhale   \n",
      "4  Father of the Bride Part II   \n",
      "\n",
      "                                            overview    imdb_id  \\\n",
      "0  Led by Woody, Andy's toys live happily in his ...  tt0114709   \n",
      "1  When siblings Judy and Peter discover an encha...  tt0113497   \n",
      "2  A family wedding reignites the ancient feud be...  tt0113228   \n",
      "3  Cheated on, mistreated and stepped on, the wom...  tt0114885   \n",
      "4  Just when George Banks has recovered from his ...  tt0113041   \n",
      "\n",
      "                                             tagline  \\\n",
      "0                                                NaN   \n",
      "1          Roll the dice and unleash the excitement!   \n",
      "2  Still Yelling. Still Fighting. Still Ready for...   \n",
      "3  Friends are the people who let you be yourself...   \n",
      "4  Just When His World Is Back To Normal... He's ...   \n",
      "\n",
      "                                              genres  vote_average  \\\n",
      "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...           7.7   \n",
      "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...           6.9   \n",
      "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...           6.5   \n",
      "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...           6.1   \n",
      "4                     [{'id': 35, 'name': 'Comedy'}]           5.7   \n",
      "\n",
      "   vote_count  user_id  item_id   topic_1   topic_2   topic_3   topic_4  \\\n",
      "0      5415.0        0        0  0.307708  0.020474  0.020470  0.020474   \n",
      "1      2413.0        1        1  0.016770  0.192589  0.016767  0.016762   \n",
      "2        92.0        2        2  0.017812  0.017846  0.017867  0.017844   \n",
      "3        34.0        3        3  0.020928  0.020932  0.020928  0.020929   \n",
      "4       173.0        4        4  0.020019  0.020019  0.020018  0.020018   \n",
      "\n",
      "    topic_5   topic_6   topic_7   topic_8   topic_9  topic_10  \n",
      "0  0.020480  0.020473  0.528497  0.020475  0.020480  0.020470  \n",
      "1  0.016764  0.673280  0.016768  0.016765  0.016770  0.016765  \n",
      "2  0.017803  0.017817  0.326748  0.018026  0.530436  0.017800  \n",
      "3  0.020924  0.266000  0.566577  0.020924  0.020927  0.020931  \n",
      "4  0.020018  0.020018  0.819838  0.020017  0.020019  0.020017  \n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка данных: выберем обзоры\n",
    "overviews = movies_3['overview'].dropna()\n",
    "\n",
    "# 2. Преобразование текста в TF-IDF матрицу\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(overviews)\n",
    "\n",
    "# 3. Обучение модели LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)  # выбрано 10 тем\n",
    "lda_topics = lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "# 4. Посмотрим на топ слова для каждой темы\n",
    "n_words = 10  # количество слов для вывода в каждой теме\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Тема #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# 5. Создаем новые признаки для фильмов на основе тем\n",
    "topic_columns = [f'topic_{i+1}' for i in range(lda.n_components)]\n",
    "topics_df = pd.DataFrame(lda_topics, columns=topic_columns)\n",
    "\n",
    "# 6. Добавляем их в исходный DataFrame\n",
    "movies_3 = pd.concat([movies_3, topics_df], axis=1)\n",
    "\n",
    "print(movies_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7e168c27-5b3e-4a8c-a626-f675e2e0e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_recommendations(imdb_id, movies, movies_metadata, imdb_recs_cache, top_k=10, allow_scraping=False):\n",
    "    movie_index = movies[movies['imdb_id'] == imdb_id].index[0]\n",
    "    movie_overview = movies.loc[movie_index, 'overview']\n",
    "    \n",
    "    # Преобразуем обзор фильма в TF-IDF вектор\n",
    "    movie_tfidf = tfidf_vectorizer.transform([movie_overview])  # Используем глобальный tfidf_vectorizer\n",
    "    \n",
    "    # Получаем распределение тем для этого фильма\n",
    "    movie_topics = lda.transform(movie_tfidf)[0]\n",
    "    \n",
    "    # Получаем распределения тем для всех фильмов (перед тем, как вычислять сходство)\n",
    "    all_movie_topics = lda.transform(tfidf_vectorizer.transform(movies['overview'].dropna()))\n",
    "    \n",
    "    # Получаем схожесть между текущим фильмом и всеми остальными фильмами\n",
    "    similarity_scores = cosine_similarity([movie_topics], all_movie_topics)[0]\n",
    "\n",
    "    # Сортируем индексы по убыванию\n",
    "    recommended_idx = similarity_scores.argsort()[::-1]\n",
    "    \n",
    "    # Убедимся, что recommended_idx имеет тип numpy.ndarray\n",
    "    recommended_idx = np.array(recommended_idx)\n",
    "    \n",
    "    # Получаем топ K элементов\n",
    "    recommended_idx = recommended_idx[:10]\n",
    "    \n",
    "    # Приводим тип индексов к int, если необходимо\n",
    "    recommended_idx = recommended_idx.astype(int)\n",
    "\n",
    "    # Извлекаем фильмы с помощью индексов\n",
    "    recommended_movies = movies.iloc[recommended_idx]\n",
    "    \n",
    "    return recommended_movies['imdb_id'].tolist()\n",
    "\n",
    "# Обертка для метода get_lda_recommendations\n",
    "def get_lda_recommendations_with_tfidf(imdb_id, movies, movies_metadata, imdb_recs_cache, top_k=10, allow_scraping=False, tfidf_vectorizer=None):\n",
    "    # Просто передаем tfidf_vectorizer в глобальную область\n",
    "    global tfidf_vectorizer_global\n",
    "    tfidf_vectorizer_global = tfidf_vectorizer\n",
    "    \n",
    "    return get_lda_recommendations(\n",
    "        imdb_id, \n",
    "        movies, \n",
    "        movies_metadata, \n",
    "        imdb_recs_cache, \n",
    "        top_k=top_k, \n",
    "        allow_scraping=allow_scraping\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f28fbf22-7ffb-4bc1-a2da-7006bb81ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [21:13<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Результаты оценки LDA:\n",
      "{'precision': 0.0162, 'recall': 0.0016, 'f1': 0.0029, 'ap': 0.0016, 'ndcg': 0.0073}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_lda = evaluate_model_als(\n",
    "    get_lda_recommendations_with_tfidf,  # Используем обертку\n",
    "    ground_truth,\n",
    "    movies_3,\n",
    "    movies_metadata,\n",
    "    user_item_csr,\n",
    "    imdb_recs_cache,\n",
    "    top_k=10,\n",
    "    is_regression=False\n",
    ")\n",
    "\n",
    "print(\"🎯 Результаты оценки LDA:\")\n",
    "print(results_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2b710-af36-4822-a502-9c7f06b9e21f",
   "metadata": {},
   "source": [
    "### Метрки хуже чем у KNN т.к опять же мтеод расчитан на более серьезную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93b653-84bc-4d08-9256-ff51d082ae16",
   "metadata": {},
   "source": [
    "## 7 Гибридная модель: TF-IDF + популярность Учет vote_average, vote_count в финальном скоринге"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ff0d3-898d-4576-a6aa-9338c385b1c5",
   "metadata": {},
   "source": [
    "Гибридная модель: TF-IDF + популярность:\n",
    "- 1. TF-IDF матрица:\n",
    "Это tfidf_matrix, например, [10000 x 5000], где каждая строка — это фильм, а каждая колонка — слово из описания.\n",
    "Она отвечает за смысловую близость.\n",
    "- 2. Популярность:\n",
    "Мы создаём числовой показатель, например:\n",
    "popularity_score = vote_average * log1p(vote_count)\n",
    "Он говорит, насколько фильм хорош по мнению аудитории.\n",
    "Мы нормализуем этот скор, чтобы он был в [0, 1], и добавляем как дополнительный признак к TF-IDF.\n",
    "- 3. Объединение признаков:\n",
    "С помощью scipy.sparse.hstack() мы объединяем sparse TF-IDF и колонку популярности:\n",
    "combined_matrix = hstack([alpha * tfidf_matrix, beta * popularity_matrix])\n",
    "Это новая матрица [10000 x (5000+1)].\n",
    "- 4. Косинусное сходство:\n",
    "Теперь мы меряем похожесть фильмов не только по смыслу, но и с поправкой на популярность:\n",
    "cosine_sim = linear_kernel(combined_matrix, combined_matrix)\n",
    "Это улучшает рекомендации: фильмы будут и похожие по теме, и любимые зрителями.\n",
    "- Почему это работает:\n",
    "TF-IDF может вернуть очень редкие, неочевидные фильмы.\n",
    "Популярность фильтрует откровенно слабые и \"нишевые\".\n",
    "Гибрид балансирует между \"релевантный по смыслу\" и \"популярный/оценённый\".\n",
    "- Можно тюнинговать веса:\n",
    "alpha = 0.9  # смысл важнее\n",
    "beta = 0.1   # но не забываем про качество\n",
    "📊 Пример:\n",
    "Ищем рекомендации к фильму \"The Lion King\":\n",
    "\n",
    "Только TF-IDF: может вернуть фильм про \"джунгли\", \"царей\", но с рейтингом 3.5 и 200 голосов.\n",
    "Только популярность: может вернуть любой популярный фильм, даже не по теме.\n",
    "Гибрид: даст и тематически близкий, и качественный.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c496260f-c1c3-480b-b3bf-ee2cff087f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_3 = movies_metadata[['title', 'overview', 'imdb_id', 'tagline' , 'genres', 'vote_average' , 'vote_count' ]].reset_index(drop=True)\n",
    "movies_3 = movies_3[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a10990-1438-4e4b-8fa6-a6588c923c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 🔧 Настройки =====\n",
    "alpha = 0.8  # Вес TF-IDF\n",
    "beta = 0.2   # Вес популярности\n",
    "top_k = 10\n",
    "\n",
    "# ===== 📥 Загрузка и подготовка данных =====\n",
    "movies_3 = movies_3.dropna(subset=['overview']).copy()\n",
    "movies_3['vote_count'] = pd.to_numeric(movies_3['vote_count'], errors='coerce').fillna(0)\n",
    "movies_3['vote_average'] = pd.to_numeric(movies_3['vote_average'], errors='coerce').fillna(0)\n",
    "movies_3['popularity_score'] = movies_3['vote_average'] * np.log1p(movies_3['vote_count'])\n",
    "\n",
    "# ===== 🧪 TF-IDF =====\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies_3['overview'])\n",
    "\n",
    "# ===== 🔢 Нормализация популярности и объединение =====\n",
    "popularity_norm = (movies_3['popularity_score'] - movies_3['popularity_score'].min()) / (movies_3['popularity_score'].max() - movies_3['popularity_score'].min())\n",
    "popularity_matrix = csr_matrix(popularity_norm.values[:, np.newaxis])  # sparse column\n",
    "\n",
    "# Объединяем TF-IDF и популярность как sparse матрицы\n",
    "combined_matrix = hstack([alpha * tfidf_matrix, beta * popularity_matrix])\n",
    "\n",
    "# ===== 🧠 Косинусная близость =====\n",
    "cosine_sim = linear_kernel(combined_matrix, combined_matrix)\n",
    "\n",
    "# ===== 🔄 Индексы =====\n",
    "index_to_id = dict(enumerate(movies_3['imdb_id']))\n",
    "id_to_index = {v: k for k, v in index_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d854ea-e98f-4f7e-adf2-87fa1039a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 🔍 Гибридная модель =====\n",
    "def hybrid_model_func(imdb_id, movies, movies_metadata, imdb_recs_cache=None, top_k=10, allow_scraping=False):\n",
    "    if imdb_id not in id_to_index:\n",
    "        print(f\"⚠️ IMDb ID не найден: {imdb_id}\")\n",
    "        return []\n",
    "    idx = id_to_index[imdb_id]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = [s for s in sim_scores if s[0] != idx][:10]\n",
    "    rec_indices = [i[0] for i in sim_scores]\n",
    "    rec_ids = [index_to_id[i] for i in rec_indices]\n",
    "    return rec_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23997fe5-497c-487a-9c9e-f818d49ea45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Оценка модели: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 990/990 [00:11<00:00, 84.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Результаты гибридной модели: {'precision': 0.1414, 'recall': 0.0188, 'f1': 0.0325, 'ap': 0.0188, 'ndcg': 0.0688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model_als(\n",
    "    model_func=hybrid_model_func,\n",
    "    ground_truth=ground_truth,\n",
    "    movies=movies_3,\n",
    "    movies_metadata=movies_3,\n",
    "    sim=None,\n",
    "    imdb_recs_cache=imdb_recs_cache,\n",
    "    top_k=10,\n",
    "    is_regression=False,\n",
    "    allow_scraping=False\n",
    ")\n",
    "\n",
    "print(\"📊 Результаты гибридной модели:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f7ceb-e1f5-4734-875a-67002d768daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2. Популярность: vote_average * log(vote_count) =====\n",
    "vote_count = movies_3['vote_count'].fillna(0)\n",
    "vote_average = movies_3['vote_average'].fillna(0)\n",
    "popularity_score = vote_average * np.log1p(vote_count)\n",
    "\n",
    "# Масштабируем\n",
    "scaler = MinMaxScaler()\n",
    "popularity_scaled = scaler.fit_transform(popularity_score.values.reshape(-1, 1))\n",
    "\n",
    "# ===== 3. Гибрид: TF-IDF + популярность =====\n",
    "# Трансформируем популярность в sparse формат такой же формы, как tfidf_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "popularity_sparse = csr_matrix(np.repeat(popularity_scaled, tfidf_matrix.shape[1], axis=1))\n",
    "\n",
    "combined_matrix = alpha * tfidf_matrix + beta * popularity_sparse\n",
    "\n",
    "# ===== 4. PCA =====\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(combined_matrix.toarray())\n",
    "\n",
    "# ===== 5. Визуализация =====\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1],\n",
    "            c=popularity_scaled.flatten(),  \n",
    "            cmap='viridis', s=20, alpha=0.7)\n",
    "plt.colorbar(label='Популярность (норм.)')\n",
    "plt.title('📽️ Визуализация фильмов: TF-IDF + Популярность')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091131f-5930-411c-ae7d-5661a41cf632",
   "metadata": {},
   "source": [
    "### Самый улчшие пока на данный момент метрики. Что также не удивительно т.к это игбриный метод. т.е сочеатние несоклких методов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f74826-82e5-4d92-bd60-a533527aa505",
   "metadata": {},
   "source": [
    "## 8  Понижение размерности PCA/Autoencoder (на TF-IDF) и обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bea3f-154a-4350-99e5-64ee7a336416",
   "metadata": {},
   "source": [
    "Шаг 1: Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd219899-baf0-4079-86f6-1d7b70913843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "movies_metadata = pd.read_csv('content/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "# Обрезаем до первых 10000 строк для уменьшения нагрузки на память\n",
    "movies_metadata = movies_metadata[:10000]\n",
    "\n",
    "# Преобразуем числовые признаки с помощью Min-Max нормализации\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Преобразуем 'vote_average', 'popularity', 'vote_count' в числовой формат\n",
    "movies_metadata['vote_average'] = pd.to_numeric(movies_metadata['vote_average'], errors='coerce')\n",
    "movies_metadata['popularity'] = pd.to_numeric(movies_metadata['popularity'], errors='coerce')\n",
    "movies_metadata['vote_count'] = pd.to_numeric(movies_metadata['vote_count'], errors='coerce')\n",
    "\n",
    "# Применяем Min-Max нормализацию\n",
    "movies_metadata[['vote_average', 'popularity', 'vote_count']] = scaler.fit_transform(\n",
    "    movies_metadata[['vote_average', 'popularity', 'vote_count']])\n",
    "\n",
    "# Преобразование категориальных признаков (например, genres и tagline)\n",
    "movies_metadata['genres'] = movies_metadata['genres'].apply(lambda x: x.split(',') if isinstance(x, str) else [])\n",
    "movies_metadata['tagline'] = movies_metadata['tagline'].fillna('')\n",
    "\n",
    "# Преобразование tagline в числовые признаки с использованием CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "tagline_features = count_vectorizer.fit_transform(movies_metadata['tagline'])\n",
    "\n",
    "# Преобразование overview в TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "overview_tfidf = tfidf_vectorizer.fit_transform(movies_metadata['overview'].fillna(''))\n",
    "\n",
    "# Получаем список уникальных жанров\n",
    "all_genres = movies_metadata['genres'].dropna().explode().unique()\n",
    "\n",
    "# Создаем словарь для индексации жанров\n",
    "genre_to_index = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "\n",
    "# Преобразуем жанры в индексы\n",
    "movies_metadata['genre_indices'] = movies_metadata['genres'].apply(lambda genres: [genre_to_index[genre] for genre in genres])\n",
    "\n",
    "# Подготовка числовых признаков\n",
    "numerical_features = np.hstack([movies_metadata[['vote_average', 'popularity', 'vote_count']].values, tagline_features.toarray()])\n",
    "final_features = np.hstack([numerical_features, overview_tfidf.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579ad886-1d45-43d3-b942-a1e2cc7f8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test = train_test_split(final_features, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07409fea-4326-4d5b-a8d7-684e13d90174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11817)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba5e62-8aeb-48cc-95fe-1889f229fec8",
   "metadata": {},
   "source": [
    "Шаг 2: Создание модели Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13d1ced-f231-45bc-894d-78b3c0161084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 234ms/step - loss: 0.2169 - val_loss: 3.7232e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.6673e-04 - val_loss: 3.6297e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - loss: 3.7264e-04 - val_loss: 3.6291e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 220ms/step - loss: 3.7387e-04 - val_loss: 3.6288e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 215ms/step - loss: 3.7038e-04 - val_loss: 3.6288e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 226ms/step - loss: 3.7251e-04 - val_loss: 3.6293e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7420e-04 - val_loss: 3.6302e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 220ms/step - loss: 3.7238e-04 - val_loss: 3.6288e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.6940e-04 - val_loss: 3.6289e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - loss: 3.7092e-04 - val_loss: 3.6290e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.8401e-04 - val_loss: 3.6289e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - loss: 3.7428e-04 - val_loss: 3.6301e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - loss: 3.7748e-04 - val_loss: 3.6289e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.7251e-04 - val_loss: 3.6290e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 227ms/step - loss: 3.7154e-04 - val_loss: 3.6288e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.7237e-04 - val_loss: 3.6315e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 227ms/step - loss: 3.6935e-04 - val_loss: 3.6303e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - loss: 3.7180e-04 - val_loss: 3.6290e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.7488e-04 - val_loss: 3.6303e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 221ms/step - loss: 3.7347e-04 - val_loss: 3.6299e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7108e-04 - val_loss: 3.6292e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - loss: 3.7354e-04 - val_loss: 3.6291e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.6820e-04 - val_loss: 3.6287e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - loss: 3.6775e-04 - val_loss: 3.6290e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - loss: 3.7057e-04 - val_loss: 3.6290e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 223ms/step - loss: 3.7197e-04 - val_loss: 3.6308e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.7575e-04 - val_loss: 3.6290e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7504e-04 - val_loss: 3.6294e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.7164e-04 - val_loss: 3.6299e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 220ms/step - loss: 3.6860e-04 - val_loss: 3.6292e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 215ms/step - loss: 3.7048e-04 - val_loss: 3.6289e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - loss: 3.7800e-04 - val_loss: 3.6287e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7278e-04 - val_loss: 3.6290e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.7931e-04 - val_loss: 3.6292e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 220ms/step - loss: 3.6988e-04 - val_loss: 3.6299e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - loss: 3.7299e-04 - val_loss: 3.6287e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 214ms/step - loss: 3.7162e-04 - val_loss: 3.6293e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7823e-04 - val_loss: 3.6299e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 219ms/step - loss: 3.7135e-04 - val_loss: 3.6292e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - loss: 3.7122e-04 - val_loss: 3.6292e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 221ms/step - loss: 3.7471e-04 - val_loss: 3.6287e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - loss: 3.7236e-04 - val_loss: 3.6289e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 215ms/step - loss: 3.6954e-04 - val_loss: 3.6289e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 217ms/step - loss: 3.7141e-04 - val_loss: 3.6293e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 224ms/step - loss: 3.7239e-04 - val_loss: 3.6291e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - loss: 3.7379e-04 - val_loss: 3.6290e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7079e-04 - val_loss: 3.6287e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - loss: 3.7276e-04 - val_loss: 3.6309e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - loss: 3.6933e-04 - val_loss: 3.6295e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - loss: 3.7565e-04 - val_loss: 3.6304e-04\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Размерность обучающей выборки после сжатия: (8000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Определение модели Autoencoder\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Кодировщик (сжатие данных)\n",
    "encoded = Dense(500, activation='relu')(input_layer)\n",
    "encoded = Dense(100, activation='relu')(encoded)  # Сжимающий слой\n",
    "\n",
    "# Декодировщик (восстановление данных)\n",
    "decoded = Dense(500, activation='relu')(encoded)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(decoded)  # Восстановление до исходного размера\n",
    "\n",
    "# Модель Autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Компиляция модели\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Обучение модели Autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Получаем сжатые данные\n",
    "encoder = Model(input_layer, encoded)  # Модель для получения сжатых признаков\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "print(\"Размерность обучающей выборки после сжатия:\", X_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4fef013-7177-43b1-b919-92b6accd0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Путь к файлу для сохранения \n",
    "# X_TRAIN_ENCODE_PATH = 'content/recomend/X_train_encoded.pkl'\n",
    "# X_TEST_ENCODE_PATH = 'content/recomend/X_test_encoded.pkl'\n",
    "\n",
    "# # Проверка существования директории и её создание, если не существует\n",
    "# cache_dir = os.path.dirname(X_TRAIN_ENCODE_PATH)\n",
    "# if not os.path.exists(cache_dir):\n",
    "#     os.makedirs(cache_dir)\n",
    "# with open(X_TRAIN_ENCODE_PATH, 'wb') as f:\n",
    "#     dill.dump(X_train_encoded, f)\n",
    "\n",
    "# # Проверка существования директории и её создание, если не существует\n",
    "# cache_dir = os.path.dirname(X_TEST_ENCODE_PATH)\n",
    "# if not os.path.exists(cache_dir):\n",
    "#     os.makedirs(cache_dir)\n",
    "# with open(X_TEST_ENCODE_PATH, 'wb') as f:\n",
    "#     dill.dump(X_test_encoded, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059b4b24-d5de-42a3-aa2a-a8b4fbe3e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_ENCODE_PATH = 'content/recomend/X_train_encoded.pkl'\n",
    "with open(X_TRAIN_ENCODE_PATH, 'rb') as f:\n",
    "        X_train_encoded = dill.load(f)\n",
    "\n",
    "X_TEST_ENCODE_PATH = 'content/recomend/X_test_encoded.pkl'\n",
    "with open(X_TEST_ENCODE_PATH, 'rb') as f:\n",
    "        X_test_encoded = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0feec-9467-4bb7-ba00-3ff8e4a833f1",
   "metadata": {},
   "source": [
    "Шаг 3: Обучение модели на сжатых данных и предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8b7ea2b-c05b-472d-9742-8e53bc756c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58664a2a-63a7-4646-b307-c0cf4a12e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_train_filtered: (981, 100)\n",
      "Размер X_test_filtered: (10, 100)\n",
      "Размер y_train_filtered: (981, 990)\n",
      "Размер y_test_filtered: (10, 990)\n"
     ]
    }
   ],
   "source": [
    "# Возьмем только те фильмы, для которых есть ground_truth (идеальные рекомендации)\n",
    "ground_truth_keys = list(ground_truth.keys())\n",
    "\n",
    "# Создание бинарных меток для фильмов в ground_truth\n",
    "def create_binary_vector(imdb_id, ground_truth_keys, ground_truth):\n",
    "    return [1 if movie_id in ground_truth.get(imdb_id, []) else 0 for movie_id in ground_truth_keys]\n",
    "\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки, учитывая фильмы из ground_truth\n",
    "train_indices = [i for i, imdb_id in enumerate(movies_metadata['imdb_id'][:10000]) if imdb_id in ground_truth_keys]\n",
    "test_indices = [i for i, imdb_id in enumerate(movies_metadata['imdb_id'][:10000]) if imdb_id in ground_truth_keys and i not in train_indices]\n",
    "\n",
    "# Если в test_indices пусто, можно перенести часть фильмов из train_indices в test_indices\n",
    "if len(test_indices) == 0:\n",
    "    # Допустим, перемещаем 10 фильмов из train в test\n",
    "    test_indices = train_indices[:10]\n",
    "    train_indices = train_indices[10:]\n",
    "\n",
    "# Извлекаем только фильмы, которые есть в ground_truth\n",
    "X_train_filtered = X_train_encoded[train_indices]\n",
    "X_test_filtered = X_test_encoded[test_indices]\n",
    "\n",
    "# Проверка размерностей\n",
    "print(f\"Размер X_train_filtered: {X_train_filtered.shape}\")\n",
    "print(f\"Размер X_test_filtered: {X_test_filtered.shape}\")\n",
    "\n",
    "# Создание бинарных меток для фильмов в ground_truth\n",
    "y_train_filtered = np.array([create_binary_vector(movies_metadata['imdb_id'][i], ground_truth_keys, ground_truth) for i in train_indices])\n",
    "y_test_filtered = np.array([create_binary_vector(movies_metadata['imdb_id'][i], ground_truth_keys, ground_truth) for i in test_indices])\n",
    "\n",
    "# Проверка размерностей\n",
    "print(f\"Размер y_train_filtered: {y_train_filtered.shape}\")\n",
    "print(f\"Размер y_test_filtered: {y_test_filtered.shape}\")\n",
    "\n",
    "# Преобразуем y_train и y_test в one-hot формат\n",
    "y_train_one_hot = y_train_filtered\n",
    "y_test_one_hot = y_test_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27a6827-78d7-4d3d-8a9d-f8bb0ee05db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления NDCG\n",
    "def ndcg_at_k(y_true, y_pred, k=10):\n",
    "    order = np.argsort(y_pred, axis=1)[:, ::-1]  # Порядок сортировки предсказанных значений\n",
    "    y_true = np.array(y_true)\n",
    "    dcg = np.sum((y_true[np.arange(y_true.shape[0]), order[:, :k]] > 0).astype(int) / np.log2(np.arange(2, k + 2)))\n",
    "    idcg = np.sum((y_true[np.arange(y_true.shape[0]), order[:, :k]] > 0).astype(int) / np.log2(np.arange(2, k + 2)))\n",
    "    return np.mean(dcg / idcg)  # Нормализованный DCG\n",
    "\n",
    "# Определение кастомных метрик для Keras\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Приводим y_true к типу float32\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Приводим y_pred к типу float32\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.round(y_true), tf.round(y_pred)), tf.float32))\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.round(y_true), tf.round(y_pred)), tf.float32))\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd5a1878-67d8-4df4-a88b-225b6b960193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saatarko/.conda/envs/HomeworkDS/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - f1: 0.6977 - loss: 0.5835 - precision: 0.6977 - recall: 0.6977 - val_accuracy: 0.0000e+00 - val_f1: 0.9946 - val_loss: 0.0787 - val_precision: 0.9946 - val_recall: 0.9946\n",
      "Epoch 2/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - f1: 0.9978 - loss: 0.0381 - precision: 0.9978 - recall: 0.9978 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0175 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 3/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - f1: 0.9984 - loss: 0.0143 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0164 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 4/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 5.5187e-04 - f1: 0.9985 - loss: 0.0129 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0155 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 5/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 2.6128e-04 - f1: 0.9984 - loss: 0.0128 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0152 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 6/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - f1: 0.9985 - loss: 0.0123 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0151 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 7/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0012 - f1: 0.9984 - loss: 0.0124 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0149 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 8/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - f1: 0.9984 - loss: 0.0125 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0148 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 9/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 6.0613e-04 - f1: 0.9984 - loss: 0.0129 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0147 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 10/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0090 - f1: 0.9984 - loss: 0.0125 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0148 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 11/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - f1: 0.9984 - loss: 0.0121 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0148 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "Epoch 12/100\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0012 - f1: 0.9984 - loss: 0.0122 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.0000e+00 - val_f1: 0.9977 - val_loss: 0.0148 - val_precision: 0.9977 - val_recall: 0.9977\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - f1: 0.9977 - loss: 0.0148 - precision: 0.9977 - recall: 0.9977\n",
      "Точность: 0.0\n",
      "Precision: 0.9976767897605896\n",
      "Recall: 0.9976767897605896\n",
      "F1: 0.9976767301559448\n"
     ]
    }
   ],
   "source": [
    "# Создание модели\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_filtered.shape[1]))  # Входной слой с меньшим количеством нейронов\n",
    "model.add(Dense(32, activation='relu'))  # Промежуточный слой\n",
    "model.add(Dense(y_train_one_hot.shape[1], activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Обучение модели с меньшим количеством эпох\n",
    "X_train_filtered_small = X_train_filtered[:100]\n",
    "y_train_one_hot_small = y_train_one_hot[:100]\n",
    "\n",
    "model.fit(X_train_filtered, y_train_one_hot, epochs=100, batch_size=32, validation_data=(X_test_filtered, y_test_one_hot), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Оценка модели\n",
    "loss, accuracy, precision_val, recall_val, f1_val = model.evaluate(X_test_filtered, y_test_one_hot)\n",
    "print(f'Точность: {accuracy}')\n",
    "print(f'Precision: {precision_val}')\n",
    "print(f'Recall: {recall_val}')\n",
    "print(f'F1: {f1_val}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f39be-2b40-45c2-9d22-e442d3ef61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предположим, что они хранятся в `X_additional_features`\n",
    "X_additional_features = np.array([...])  # Пример дополнительных признаков\n",
    "\n",
    "# Объединяем сжатыми признаками из Autoencoder и дополнительными признаками\n",
    "X_train_encoded_combined = np.concatenate([X_train_encoded, X_additional_features[:X_train_encoded.shape[0]]], axis=1)\n",
    "X_test_encoded_combined = np.concatenate([X_test_encoded, X_additional_features[X_train_encoded.shape[0]:]], axis=1)\n",
    "\n",
    "# Проверка размерности\n",
    "print(\"Размерность X_train после объединения:\", X_train_encoded_combined.shape)\n",
    "print(\"Размерность X_test после объединения:\", X_test_encoded_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a834efa-6c3c-4e97-bd90-5b2267454be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Средний Precision@10: 0.0041\n",
      "📊 Средний Recall@10: 0.0041\n"
     ]
    }
   ],
   "source": [
    "k = 10  # топ-k рекомендаций\n",
    "\n",
    "eval_imdb_ids = list(ground_truth.keys())\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for imdb_id in eval_imdb_ids:\n",
    "    try:\n",
    "        # Находим индекс фильма\n",
    "        idx = movies_metadata[movies_metadata['imdb_id'] == imdb_id].index[0]\n",
    "        \n",
    "        # Получаем вектор признаков и предсказания\n",
    "        x_input = X_train_encoded[idx].reshape(1, -1)\n",
    "        \n",
    "        # Получение предсказаний без вывода\n",
    "        predicted_scores = model.predict(x_input, verbose=0).flatten()\n",
    "\n",
    "        # Получаем индексы top-k предсказанных фильмов\n",
    "        top_k_indices = np.argsort(predicted_scores)[-k:][::-1]\n",
    "        predicted_ids = [movies_metadata['imdb_id'][i] for i in top_k_indices]\n",
    "\n",
    "        # Ground truth для текущего фильма\n",
    "        true_recs = ground_truth[imdb_id]\n",
    "\n",
    "        # Binary vector: 1, если фильм есть в ground_truth\n",
    "        y_true = [1 if imdb in true_recs else 0 for imdb in predicted_ids]\n",
    "\n",
    "        # Метрики\n",
    "        precisions.append(np.sum(y_true) / k)\n",
    "        recalls.append(np.sum(y_true) / len(true_recs))  # обычно 10\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для {imdb_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n📊 Средний Precision@{k}: {np.mean(precisions):.4f}\")\n",
    "print(f\"📊 Средний Recall@{k}: {np.mean(recalls):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f69ab6-5cb2-4ef0-a639-3f164aba83c4",
   "metadata": {},
   "source": [
    "### Показатели очень слабые но сам энкодер былсделан без подора параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e4ee9-33e4-4380-ab8d-bbc22e7c1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Шаг 4: Кластеризация на сжатых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9c2258-31ff-43e0-b235-b2b4fec09c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "movies_metadata = pd.read_csv('content/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "# Обрезаем до первых 10000 строк для уменьшения нагрузки на память\n",
    "movies_metadata = movies_metadata[:10000]\n",
    "\n",
    "# Преобразуем числовые признаки с помощью Min-Max нормализации\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Преобразуем 'vote_average', 'popularity', 'vote_count' в числовой формат\n",
    "movies_metadata['vote_average'] = pd.to_numeric(movies_metadata['vote_average'], errors='coerce')\n",
    "movies_metadata['popularity'] = pd.to_numeric(movies_metadata['popularity'], errors='coerce')\n",
    "movies_metadata['vote_count'] = pd.to_numeric(movies_metadata['vote_count'], errors='coerce')\n",
    "\n",
    "# Применяем Min-Max нормализацию\n",
    "movies_metadata[['vote_average', 'popularity', 'vote_count']] = scaler.fit_transform(\n",
    "    movies_metadata[['vote_average', 'popularity', 'vote_count']])\n",
    "\n",
    "# Преобразование категориальных признаков (например, genres и tagline)\n",
    "movies_metadata['genres'] = movies_metadata['genres'].apply(lambda x: x.split(',') if isinstance(x, str) else [])\n",
    "movies_metadata['tagline'] = movies_metadata['tagline'].fillna('')\n",
    "\n",
    "# Преобразование tagline в числовые признаки с использованием CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "tagline_features = count_vectorizer.fit_transform(movies_metadata['tagline'])\n",
    "\n",
    "# Преобразование overview в TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "overview_tfidf = tfidf_vectorizer.fit_transform(movies_metadata['overview'].fillna(''))\n",
    "\n",
    "# Получаем список уникальных жанров\n",
    "all_genres = movies_metadata['genres'].dropna().explode().unique()\n",
    "\n",
    "# Создаем словарь для индексации жанров\n",
    "genre_to_index = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "\n",
    "# Преобразуем жанры в индексы\n",
    "movies_metadata['genre_indices'] = movies_metadata['genres'].apply(lambda genres: [genre_to_index[genre] for genre in genres])\n",
    "\n",
    "# Подготовка числовых признаков\n",
    "numerical_features = np.hstack([movies_metadata[['vote_average', 'popularity', 'vote_count']].values, tagline_features.toarray()])\n",
    "final_features = np.hstack([numerical_features, overview_tfidf.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c8007-0393-41f1-b961-b0cdce4b2dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035dbd42-b5f9-4e67-aceb-bbe9569bc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ресэмплинг данных для ускорения кластеризации\n",
    "X_sample = resample(final_features, n_samples=2000, random_state=42)\n",
    "\n",
    "# Иерархическая кластеризация на подмножестве данных\n",
    "agg_clustering = AgglomerativeClustering(distance_threshold=0.5, n_clusters=None)\n",
    "clusters_sample = agg_clustering.fit_predict(X_sample)\n",
    "\n",
    "# Получаем ссылки на кластеры из агломеративного дерева\n",
    "linkage_matrix = agg_clustering.children_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a2bb55-add3-4c7c-a3e8-840a53bfac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0       6809\n",
      "2        177\n",
      "510      114\n",
      "85       100\n",
      "1503      60\n",
      "        ... \n",
      "1314       1\n",
      "1529       1\n",
      "477        1\n",
      "1066       1\n",
      "1661       1\n",
      "Name: count, Length: 1793, dtype: int64\n",
      "                         title  cluster\n",
      "0                    Toy Story        0\n",
      "1                      Jumanji       92\n",
      "2             Grumpier Old Men       85\n",
      "3            Waiting to Exhale     1083\n",
      "4  Father of the Bride Part II     1411\n"
     ]
    }
   ],
   "source": [
    "# Шаг 2: Применяем ближайших соседей для всех точек X_train_encoded, чтобы найти кластер для каждой точки\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X_sample)\n",
    "distances, indices = nbrs.kneighbors(final_features)\n",
    "\n",
    "# Теперь каждому элементу из X_train_encoded мы присваиваем кластер ближайшего соседа из X_sample\n",
    "clusters_all = clusters_sample[indices.flatten()]\n",
    "\n",
    "# Добавляем кластеры в основной набор данных\n",
    "movies_metadata['cluster'] = clusters_all\n",
    "\n",
    "# Проверим количество фильмов в каждом кластере\n",
    "print(movies_metadata['cluster'].value_counts())\n",
    "\n",
    "# Выводим пример распределения кластеров\n",
    "print(movies_metadata[['title', 'cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2852d0f9-0337-4950-82a9-9a80030b845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0045\n",
      "Recall: 0.0045\n",
      "F1-Score: 0.0045\n"
     ]
    }
   ],
   "source": [
    "# Функция для получения истинных рекомендаций (например, из ground_truth)\n",
    "def get_true_recommendations(movie_id):\n",
    "    return ground_truth.get(movie_id, [])\n",
    "    \n",
    "# Функция для получения рекомендаций по кластеру\n",
    "def recommend_by_cluster(movie_id, num_recommendations=5):\n",
    "    # Убедимся, что movie_id передается в нужном формате\n",
    "    movie_id = movie_id.strip()  # Удаляем пробелы вокруг\n",
    "    if movie_id not in movies_metadata['imdb_id'].values:\n",
    "        print(f\"Movie ID {movie_id} not found in movies_metadata.\")\n",
    "        return []  # Возвращаем пустой список, если не нашли\n",
    "    # Получаем кластер\n",
    "    cluster = movies_metadata.loc[movies_metadata['imdb_id'] == movie_id, 'cluster'].values[0]\n",
    "    cluster_movies = movies_metadata[movies_metadata['cluster'] == cluster]\n",
    "    recommended_movies = cluster_movies['imdb_id'].values[:num_recommendations]  # Рекомендуем фильмы из того же кластера\n",
    "    return recommended_movies\n",
    "\n",
    "# Оценка на ground_truth\n",
    "y_true = []  # Истинные рекомендации\n",
    "y_pred = []  # Предсказанные рекомендации\n",
    "\n",
    "for movie_id in ground_truth:\n",
    "    true_recommendations = get_true_recommendations(movie_id)  # Получаем реальные рекомендации\n",
    "    predicted_recommendations = recommend_by_cluster(movie_id)  # Получаем рекомендованные фильмы по кластеру\n",
    "    \n",
    "    # Проверяем, что список рекомендованных фильмов не пуст\n",
    "    if len(predicted_recommendations) > 0:\n",
    "        # Заполняем список истинных и предсказанных рекомендаций\n",
    "        y_true.extend([1 if imdb in true_recommendations else 0 for imdb in predicted_recommendations])  # бинаризация\n",
    "        y_pred.extend([1 if imdb in predicted_recommendations else 0 for imdb in predicted_recommendations])\n",
    "\n",
    "# Рассчитываем Precision, Recall и F1-Score\n",
    "precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e087c92a-83f1-418b-952c-1d6116a66cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.0045\n"
     ]
    }
   ],
   "source": [
    "def calculate_map(y_true, y_pred):\n",
    "    return average_precision_score(y_true, y_pred, average='micro')\n",
    "\n",
    "map_score = calculate_map(y_true, y_pred)\n",
    "print(f'Mean Average Precision (MAP): {map_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edffc094-cfc0-4723-8bf9-fdbf1a77c4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def ndcg_at_k(y_true, y_pred, k=5):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    for i in range(k):\n",
    "        if y_true[i] == 1:\n",
    "            dcg += 1 / np.log2(i + 2)  # +2 because ranks are 1-based\n",
    "        if sorted(y_true, reverse=True)[i] == 1:\n",
    "            idcg += 1 / np.log2(i + 2)\n",
    "    return dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "ndcg_score = ndcg_at_k(y_true, y_pred, k=5)\n",
    "print(f'NDCG@5: {ndcg_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ec524-44d2-4286-baff-5f19f4c1fcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
